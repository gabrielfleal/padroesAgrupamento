{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import string\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aqui você tem que por o diretório da pasta que tem todos os documentos:\n",
    "DIR = \"C:\\\\Users\\Marília\\\\Desktop\\\\python_experience\\\\padroesAgrupamento\\\\alldocs\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in os.walk(DIR):\n",
    "    d = files\n",
    "documents = d[2][:]\n",
    "\n",
    "terms = [\n",
    "    \"carbohydr\",\n",
    "    \"oligosaccharid\",\n",
    "    \"glycosyl\",\n",
    "    \"glycoprotein\",\n",
    "    \"diseas\",\n",
    "    \"cell\",\n",
    "    \"develop\",\n",
    "    \"In\",\n",
    "    \"site\",\n",
    "    \"bind\",\n",
    "    \"express\",\n",
    "    \"structur\",\n",
    "    \"result\",\n",
    "    \"analysi\",\n",
    "    \"one\",\n",
    "    \"cancer\",\n",
    "    \"protein\",\n",
    "    \"signal\",\n",
    "    \"assay\",\n",
    "    \"regul\",\n",
    "    \"activ\",\n",
    "    \"diseas\",\n",
    "    \"effect\",\n",
    "    \"respons\",\n",
    "    \"pathway\",\n",
    "    \"express\",\n",
    "    \"therapeut\",\n",
    "    \"increas\",\n",
    "    \"inhibit\",\n",
    "    \"patient\",\n",
    "    \"hta\",\n",
    "    \"technolog\",\n",
    "    \"percent\",\n",
    "    \"implement\",\n",
    "    \"compar\",\n",
    "    \"system\",\n",
    "    \"evalu\",\n",
    "    \"recommend\",\n",
    "    \"review\",\n",
    "    \"activ\",\n",
    "    \"evid\",\n",
    "    \"report\",\n",
    "    \"improv\",\n",
    "    \"research\",\n",
    "    \"morphin\",\n",
    "    \"mobil\",\n",
    "    \"signific\",\n",
    "    \"naloxon\",\n",
    "    \"rate\",\n",
    "    \"report\",\n",
    "    \"epidur\",\n",
    "    \"time\",\n",
    "    \"effect\",\n",
    "    \"found\",\n",
    "    \"pain\",\n",
    "    \"countri\",\n",
    "    \"the\",\n",
    "    \"care\",\n",
    "    \"least\",\n",
    "    \"patient\",\n",
    "    \"featur\",\n",
    "    \"high\",\n",
    "    \"detect\",\n",
    "    \"measur\",\n",
    "    \"clinic\",\n",
    "    \"annot\",\n",
    "    \"learn\",\n",
    "    \"system\",\n",
    "    \"diseas\",\n",
    "    \"care\",\n",
    "    \"health\",\n",
    "    \"impact\",\n",
    "    \"cancer\",\n",
    "    \"concept\",\n",
    "    \"countri\",\n",
    "    \"univers\",\n",
    "    \"health\",\n",
    "    \"system\",\n",
    "    \"organiz\",\n",
    "    \"manag\",\n",
    "    \"applic\",\n",
    "    \"systemat\",\n",
    "    \"care\",\n",
    "    \"scienc\",\n",
    "    \"data\",\n",
    "    \"commun\",\n",
    "    \"inform\",\n",
    "    \"provid\",\n",
    "    \"sequenc\",\n",
    "    \"analysi\",\n",
    "    \"tool\",\n",
    "    \"genom\",\n",
    "    \"method\",\n",
    "    \"variant\",\n",
    "    \"gene\",\n",
    "    \"associ\",\n",
    "    \"studi\",\n",
    "    \"model\",\n",
    "    \"packag\",\n",
    "    \"develop\",\n",
    "    \"result\",\n",
    "    \"algorithm\",\n",
    "    \"perform\",\n",
    "]\n",
    "\n",
    "terms = list(set(terms))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullText = []\n",
    "\n",
    "#Gambiarra to read Mayandre's files\n",
    "Mayandre = [\n",
    "    \"10MY.txt\",\n",
    "    \"3MY.txt\",\n",
    "    \"4MY.txt\",\n",
    "    \"7MY.txt\",\n",
    "    \"8MY.txt\",\n",
    "    \"9MY.txt\"\n",
    "]\n",
    "\n",
    "for i in range(0, len(documents)):\n",
    "\n",
    "    #####################################################################################################\n",
    "    #Gambiarra's start delimiter\n",
    "    test = True\n",
    "    \n",
    "    for j in Mayandre:\n",
    "        if documents[i] == j:\n",
    "            file = open(DIR + documents[i], \"r\", encoding = \"utf-16\")\n",
    "            fullText.append(file.read())\n",
    "            test = False\n",
    "    \n",
    "    if test:\n",
    "        #####################################################################################################\n",
    "        #Gambiarra's end delimiter\n",
    "        file = open(DIR + documents[i], \"r\")\n",
    "        fullText.append(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = set(stopwords.words('english'))\n",
    "stopWords.add(\"Abstract\")\n",
    "stopWords.add(\"Review\")\n",
    "stopWords.add(\"Results\")\n",
    "stopWords.add(\"Conclusions\")\n",
    "stopWords.add(\"Methods\")\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "finalList = []\n",
    "\n",
    "for i in range(0,len(documents)):\n",
    "\n",
    "    words = word_tokenize(fullText[i])\n",
    "    processedWords = []\n",
    "    for word in words:\n",
    "        if word not in stopWords and word not in punctuations:\n",
    "            processedWords.append(ps.stem(word))\n",
    "    \n",
    "    finalList.append(processedWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqRel = []\n",
    "palavrasPorDoc = []\n",
    "\n",
    "for i in range(0, len(finalList)):\n",
    "    novaLista = list(set(finalList[i]))\n",
    "    palavrasPorDoc.append(novaLista)\n",
    "    \n",
    "    outraLista = []\n",
    "    for j in terms:\n",
    "        if finalList[i].count(j) > 0:\n",
    "            outraLista.append([j, 1 + math.log10(finalList[i].count(j))])\n",
    "        else:\n",
    "            outraLista.append([j, 0])\n",
    "        \n",
    "    freqRel.append(outraLista)\n",
    "    \n",
    "freqRel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = []\n",
    "\n",
    "for word in terms:\n",
    "    \n",
    "    counter = 0;\n",
    "    for i in range(0, len(finalList)):\n",
    "        if finalList[i].count(word) > 0:\n",
    "            counter = counter + 1;\n",
    "            \n",
    "    idf.append([word, math.log10((len(finalList) + 1) / counter)])\n",
    "    \n",
    "idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = []\n",
    "\n",
    "for doc in range(0, len(freqRel)):\n",
    "    temp = []\n",
    "    for i in range(0, len(freqRel[doc])):\n",
    "        for j in range(0, len(idf)):\n",
    "            if idf[j][0] == freqRel[doc][i][0]:\n",
    "                res = freqRel[doc][i][1] * idf[j][1]\n",
    "        temp.append(res)\n",
    "    tfidf.append(temp)\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from numpy import *\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "\n",
    "num_clusters = 4\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, init = 'random', max_iter = 700, n_init=100)\n",
    "\n",
    "km.fit(tfidf)\n",
    "\n",
    "y_kmeans = km.predict(tfidf)\n",
    "\n",
    "silhouette_avg = silhouette_score(tfidf, y_kmeans)\n",
    "print(\"Com o número de clusters =\", num_clusters,\"a média silhouette_score é :\", silhouette_avg)\n",
    "sample_silhouette_values = silhouette_samples(tfidf, y_kmeans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = km.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = num_clusters\n",
    "tfs_reduced = TruncatedSVD(n_components=k, random_state=0).fit_transform(tfidf)\n",
    "tfs_embedded = TSNE(n_components=2, perplexity=40).fit_transform(tfs_reduced)\n",
    "fig = plt.figure(figsize = (10, 10))\n",
    "ax = plt.axes()\n",
    "plt.scatter(tfs_embedded[:, 0], tfs_embedded[:, 1], marker = \"x\", c = km.labels_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(km.labels_, bins=k)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_assignments_dict = {}\n",
    "\n",
    "for i in set(km.labels_):\n",
    "    current_cluster_bills = [documents[x] for x in np.where(km.labels_ == i)[0]]\n",
    "    cluster_assignments_dict[i] = current_cluster_bills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "remove_digits = str.maketrans('', '', digits)\n",
    "\n",
    "for cluster_id in range(0,num_clusters):\n",
    "    print ('Cluster {0}' . format(cluster_id))\n",
    "    array = []\n",
    "    for key in cluster_assignments_dict[cluster_id]:\n",
    "        array.append(((os.path.splitext(key)[0]).translate(remove_digits)))\n",
    "    print(Counter(array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distortions = []\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(tfidf)\n",
    "    kmeanModel.fit(tfidf)\n",
    "    distortions.append(sum(np.min(cdist(tfidf, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / np.array(tfidf).shape)\n",
    " \n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
